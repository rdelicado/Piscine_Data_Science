# üìä Piscine Data Science

[![Language](https://img.shields.io/badge/Language-Python-blue.svg)](https://www.python.org/)
[![Database](https://img.shields.io/badge/Database-PostgreSQL-blue.svg)](https://www.postgresql.org/)
[![Analysis](https://img.shields.io/badge/Analysis-Pandas-purple.svg)](https://pandas.pydata.org/)
[![Visualization](https://img.shields.io/badge/Visualization-Matplotlib-orange.svg)](https://matplotlib.org/)
[![Container](https://img.shields.io/badge/Container-Docker-blue.svg)](https://www.docker.com/)
[![42 School](https://img.shields.io/badge/42-School_Project-purple.svg)](https://42.fr/)

## üìã Descripci√≥n

**Piscine Data Science** es un programa intensivo de formaci√≥n en ciencia de datos que cubre los fundamentos esenciales del an√°lisis de datos, desde la gesti√≥n de bases de datos hasta la visualizaci√≥n avanzada. Este proyecto representa un recorrido completo por las tecnolog√≠as y metodolog√≠as fundamentales utilizadas en el mundo de la ciencia de datos moderna.

El programa est√° estructurado en tres m√≥dulos progresivos que abarcan SQL y bases de datos relacionales, procesamiento de datos con Python y Pandas, y visualizaci√≥n de datos con Matplotlib. Cada m√≥dulo incluye ejercicios pr√°cticos y proyectos que simulan escenarios reales de an√°lisis de datos empresariales.

### Objetivos del Programa

- **Database Management**: Dominio de SQL y gesti√≥n de bases de datos PostgreSQL
- **Data Processing**: Procesamiento y an√°lisis de datos con Python y Pandas
- **Data Visualization**: Creaci√≥n de visualizaciones efectivas con Matplotlib
- **Statistical Analysis**: An√°lisis estad√≠stico y interpretaci√≥n de resultados
- **Business Intelligence**: Aplicaci√≥n pr√°ctica en contextos empresariales
- **ETL Processes**: Extracci√≥n, transformaci√≥n y carga de datos

## üèóÔ∏è Arquitectura del Sistema

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ        Data Science Stack           ‚îÇ
                    ‚îÇ         (Docker Compose)            ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ                         ‚îÇ                         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   PostgreSQL   ‚îÇ    ‚îÇ      Python         ‚îÇ    ‚îÇ   pgAdmin4     ‚îÇ
    ‚îÇ   Database     ‚îÇ    ‚îÇ   Data Analysis     ‚îÇ    ‚îÇ  Web Interface ‚îÇ
    ‚îÇ   Port 5432    ‚îÇ    ‚îÇ  (Pandas/Matplotlib)‚îÇ    ‚îÇ   Port 8080    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                         ‚îÇ                         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   SQL Tables   ‚îÇ    ‚îÇ    Data Processing  ‚îÇ    ‚îÇ   Database     ‚îÇ
    ‚îÇ   & Schemas    ‚îÇ    ‚îÇ     & Analysis      ‚îÇ    ‚îÇ   Management   ‚îÇ
    ‚îÇ   (Customer    ‚îÇ    ‚îÇ   (Statistics &     ‚îÇ    ‚îÇ   (Visual      ‚îÇ
    ‚îÇ    Events)     ‚îÇ    ‚îÇ   Visualization)    ‚îÇ    ‚îÇ    Admin)      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ         Data Pipeline              ‚îÇ
                    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                    ‚îÇ   Data Source   ‚îÇ   Data Analysis   ‚îÇ
                    ‚îÇ   (CSV Files)   ‚îÇ   (Python/Pandas) ‚îÇ
                    ‚îÇ   Import/ETL    ‚îÇ  Visualization    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ M√≥dulos del Programa

### üìä Data Science Module 0: Database Fundamentals
- **Tecnolog√≠as**: PostgreSQL, SQL, Docker
- **Enfoque**: Fundamentos de bases de datos relacionales
- **Competencias**:
  - Dise√±o de esquemas de bases de datos
  - Consultas SQL avanzadas
  - Gesti√≥n de tipos de datos
  - Importaci√≥n y exportaci√≥n de datos
  - Optimizaci√≥n de queries
  - Administraci√≥n de bases de datos

### üîß Data Science Module 1: Data Engineering
- **Tecnolog√≠as**: Shell Scripting, PostgreSQL, CSV Processing
- **Enfoque**: Ingenier√≠a de datos y automatizaci√≥n
- **Competencias**:
  - Automatizaci√≥n con shell scripts
  - Procesos ETL (Extract, Transform, Load)
  - Gesti√≥n de archivos CSV
  - Integraci√≥n de sistemas
  - Monitoreo de procesos de datos
  - Administraci√≥n de infraestructura

### üìà Data Science Module 2: Data Analysis & Visualization
- **Tecnolog√≠as**: Python, Pandas, Matplotlib, NumPy
- **Enfoque**: An√°lisis de datos y visualizaci√≥n
- **Competencias**:
  - An√°lisis exploratorio de datos (EDA)
  - Limpieza y transformaci√≥n de datos
  - An√°lisis estad√≠stico descriptivo
  - Visualizaci√≥n de datos efectiva
  - Interpretaci√≥n de resultados
  - Comunicaci√≥n de insights

## üõ†Ô∏è Instalaci√≥n y Configuraci√≥n

### Requisitos Previos

```bash
# Docker y Docker Compose
docker --version
docker-compose --version

# Python 3.8+ (para an√°lisis local)
python3 --version

# Git para control de versiones
git --version
```

### Configuraci√≥n del Entorno

```bash
# Clonar el repositorio
git clone https://github.com/rdelicado/Piscine_Data_Science.git
cd Piscine_Data_Science

# Configurar estructura de datos
mkdir -p ~/rdelicad/data/subject
mkdir -p ~/rdelicad/data/customer

# Copiar archivos de datos (si est√°n disponibles)
cp data_files/* ~/rdelicad/data/subject/
```

### Despliegue de la Infraestructura

**Module 0 - Database Setup**:
```bash
# Navegar al m√≥dulo 0
cd data_sciencie_0

# Levantar la infraestructura PostgreSQL
make up

# Verificar servicios
docker-compose ps

# Acceder a la base de datos
make exec

# Crear tablas iniciales
psql -U rdelicad -d piscineds -f ex02/table.sql
```

**Module 1 - Data Engineering**:
```bash
# Navegar al m√≥dulo 1
cd ../data_sciencie_1

# Levantar servicios
make up

# Ejecutar scripts de carga de datos
chmod +x ex01/customers_table.sh
./ex01/customers_table.sh

# Verificar importaci√≥n de datos
make exec
```

**Module 2 - Data Analysis**:
```bash
# Navegar al m√≥dulo 2
cd ../data_sciencie_2

# Instalar dependencias Python
pip install pandas matplotlib psycopg2-binary numpy seaborn

# Ejecutar an√°lisis de ejemplo
python ex00/pie.py

# Visualizar resultados
open ex00/pie.png
```

## üöÄ Uso del Sistema

### Gesti√≥n de Base de Datos

**Conexi√≥n y Comandos B√°sicos**:
```sql
-- Conectar a la base de datos
\c piscineds

-- Listar tablas
\dt

-- Describir estructura de tabla
\d customers

-- Verificar datos importados
SELECT COUNT(*) FROM customers;
SELECT * FROM customers LIMIT 10;
```

**An√°lisis SQL Avanzado**:
```sql
-- An√°lisis de eventos por tipo
SELECT 
    event_type,
    COUNT(*) as total_events,
    AVG(price) as average_price,
    SUM(price) as total_revenue
FROM customers 
WHERE event_type = 'purchase'
GROUP BY event_type
ORDER BY total_events DESC;

-- An√°lisis temporal de actividad
SELECT 
    DATE(event_time) as event_date,
    COUNT(*) as daily_events,
    COUNT(DISTINCT user_id) as unique_users
FROM customers
GROUP BY DATE(event_time)
ORDER BY event_date;

-- Top productos por revenue
SELECT 
    product_id,
    COUNT(*) as purchase_count,
    SUM(price) as total_revenue,
    AVG(price) as avg_price
FROM customers
WHERE event_type = 'purchase'
GROUP BY product_id
ORDER BY total_revenue DESC
LIMIT 10;
```

### An√°lisis de Datos con Python

**Conexi√≥n y Carga de Datos**:
```python
import pandas as pd
import psycopg2
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Configurar conexi√≥n
def connect_db():
    return psycopg2.connect(
        dbname="piscineds",
        user="rdelicad",
        password="mysecretpassword",
        host="localhost",
        port="5432"
    )

# Cargar datos desde PostgreSQL
def load_data(query):
    conn = connect_db()
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df

# Ejemplo de carga
df = load_data("SELECT * FROM customers LIMIT 1000")
print(df.head())
print(df.info())
```

**An√°lisis Exploratorio de Datos (EDA)**:
```python
# Estad√≠sticas descriptivas
print("=== ESTAD√çSTICAS DESCRIPTIVAS ===")
print(df.describe())

# Distribuci√≥n de tipos de eventos
print("\n=== DISTRIBUCI√ìN DE EVENTOS ===")
event_dist = df['event_type'].value_counts()
print(event_dist)

# An√°lisis temporal
df['event_time'] = pd.to_datetime(df['event_time'])
df['hour'] = df['event_time'].dt.hour
df['day_of_week'] = df['event_time'].dt.day_name()

# Actividad por hora del d√≠a
hourly_activity = df.groupby('hour').size()
print("\n=== ACTIVIDAD POR HORA ===")
print(hourly_activity)

# An√°lisis de precios
purchase_data = df[df['event_type'] == 'purchase']
if not purchase_data.empty:
    print("\n=== AN√ÅLISIS DE PRECIOS ===")
    print(f"Precio promedio: ${purchase_data['price'].mean():.2f}")
    print(f"Precio mediano: ${purchase_data['price'].median():.2f}")
    print(f"Precio m√°ximo: ${purchase_data['price'].max():.2f}")
    print(f"Precio m√≠nimo: ${purchase_data['price'].min():.2f}")
```

### Visualizaci√≥n de Datos

**Gr√°ficos B√°sicos**:
```python
# Configurar estilo
plt.style.use('seaborn-v0_8')
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Distribuci√≥n de tipos de eventos (Pie Chart)
event_counts = df['event_type'].value_counts()
axes[0,0].pie(event_counts.values, labels=event_counts.index, autopct='%1.1f%%')
axes[0,0].set_title('Distribuci√≥n de Tipos de Eventos')

# 2. Actividad por hora del d√≠a (Bar Chart)
hourly_activity.plot(kind='bar', ax=axes[0,1])
axes[0,1].set_title('Actividad por Hora del D√≠a')
axes[0,1].set_xlabel('Hora')
axes[0,1].set_ylabel('N√∫mero de Eventos')

# 3. Distribuci√≥n de precios (Histogram)
if not purchase_data.empty:
    axes[1,0].hist(purchase_data['price'], bins=30, alpha=0.7)
    axes[1,0].set_title('Distribuci√≥n de Precios')
    axes[1,0].set_xlabel('Precio')
    axes[1,0].set_ylabel('Frecuencia')

# 4. Actividad por d√≠a de la semana (Bar Chart)
daily_activity = df['day_of_week'].value_counts()
daily_activity.plot(kind='bar', ax=axes[1,1])
axes[1,1].set_title('Actividad por D√≠a de la Semana')
axes[1,1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig('comprehensive_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
```

## üìÅ Estructura del Proyecto

```
Piscine_Data_Science/
‚îú‚îÄ‚îÄ README.md                             # Documentaci√≥n principal
‚îú‚îÄ‚îÄ .gitignore                           # Archivos ignorados por Git
‚îú‚îÄ‚îÄ data_sciencie_0/                     # M√≥dulo 0: Database Fundamentals
‚îÇ   ‚îú‚îÄ‚îÄ Makefile                         # Automatizaci√≥n de comandos
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml               # Configuraci√≥n PostgreSQL + pgAdmin
‚îÇ   ‚îú‚îÄ‚îÄ ex02/                            # Ejercicio: Dise√±o de tablas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ table.sql                    # Script de creaci√≥n de tablas
‚îÇ   ‚îú‚îÄ‚îÄ ex03/                            # Ejercicio: Tipos de datos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ advanced_types.sql           # Tipos de datos avanzados
‚îÇ   ‚îî‚îÄ‚îÄ ex04/                            # Ejercicio: Constraints
‚îÇ       ‚îî‚îÄ‚îÄ constraints.sql              # Restricciones y validaciones
‚îú‚îÄ‚îÄ data_sciencie_1/                     # M√≥dulo 1: Data Engineering
‚îÇ   ‚îú‚îÄ‚îÄ Makefile                         # Automatizaci√≥n de procesos
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml               # Configuraci√≥n extendida
‚îÇ   ‚îú‚îÄ‚îÄ ex01/                            # Ejercicio: Import de datos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ customers_table.sh           # Script de importaci√≥n CSV
‚îÇ   ‚îú‚îÄ‚îÄ ex02/                            # Ejercicio: ETL processes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_transform.sh            # Transformaci√≥n de datos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validation.sql               # Validaci√≥n de datos
‚îÇ   ‚îî‚îÄ‚îÄ ex03/                            # Ejercicio: Monitoring
‚îÇ       ‚îú‚îÄ‚îÄ monitor.sh                   # Script de monitoreo
‚îÇ       ‚îî‚îÄ‚îÄ health_check.sql             # Verificaci√≥n de salud
‚îî‚îÄ‚îÄ data_sciencie_2/                     # M√≥dulo 2: Data Analysis
    ‚îú‚îÄ‚îÄ Makefile                         # Automatizaci√≥n de an√°lisis
    ‚îú‚îÄ‚îÄ docker-compose.yml               # Configuraci√≥n con Jupyter
    ‚îú‚îÄ‚îÄ data_02.pdf                      # Documentaci√≥n del m√≥dulo
    ‚îú‚îÄ‚îÄ init_customers.sh                # Script de inicializaci√≥n
    ‚îú‚îÄ‚îÄ ex00/                            # Ejercicio: Visualizaci√≥n b√°sica
    ‚îÇ   ‚îú‚îÄ‚îÄ pie.py                       # Gr√°fico de torta
    ‚îÇ   ‚îî‚îÄ‚îÄ pie.png                      # Resultado visual
    ‚îî‚îÄ‚îÄ ex01/                            # Ejercicio: An√°lisis temporal
        ‚îú‚îÄ‚îÄ temporal_analysis.py         # An√°lisis de series temporales
        ‚îú‚îÄ‚îÄ trends.py                    # An√°lisis de tendencias
        ‚îî‚îÄ‚îÄ seasonality.py               # An√°lisis de estacionalidad
```

## üîß Comandos √ötiles del Makefile

```makefile
# Comandos principales para cada m√≥dulo

# Module 0
make up          # Levantar PostgreSQL + pgAdmin
make down        # Detener servicios
make exec        # Acceder al contenedor de base de datos
make logs        # Ver logs de PostgreSQL
make clean       # Limpiar vol√∫menes y datos

# Module 1
make import      # Importar datos CSV
make validate    # Validar integridad de datos
make monitor     # Monitorear procesos ETL
make backup      # Backup de base de datos
make restore     # Restaurar desde backup

# Module 2
make analyze     # Ejecutar an√°lisis completo
make visualize   # Generar todas las visualizaciones
make report      # Generar reporte HTML
make jupyter     # Levantar Jupyter Notebook
make test        # Ejecutar tests de an√°lisis
```

## üìä Casos de Uso y An√°lisis

### An√°lisis de E-commerce

**Customer Journey Analysis**:
```python
def analyze_customer_journey():
    """
    Analiza el viaje del cliente desde la primera vista hasta la compra
    """
    # Secuencia de eventos por usuario
    user_journey = df.groupby('user_id')['event_type'].apply(list).reset_index()
    user_journey['journey_length'] = user_journey['event_type'].apply(len)
    
    # Conversi√≥n view -> purchase
    conversion_users = df[df['event_type'].isin(['view', 'purchase'])].groupby('user_id')['event_type'].apply(list)
    conversions = conversion_users[conversion_users.apply(lambda x: 'view' in x and 'purchase' in x)]
    
    print(f"Usuarios con conversi√≥n: {len(conversions)}")
    print(f"Tasa de conversi√≥n: {len(conversions)/len(conversion_users)*100:.2f}%")
    
    return user_journey

# An√°lisis de abandono de carrito
def cart_abandonment_analysis():
    """
    Analiza el abandono de carrito de compras
    """
    cart_users = df[df['event_type'] == 'cart']['user_id'].unique()
    purchase_users = df[df['event_type'] == 'purchase']['user_id'].unique()
    
    abandoned_carts = set(cart_users) - set(purchase_users)
    
    print(f"Usuarios que a√±adieron al carrito: {len(cart_users)}")
    print(f"Usuarios que compraron: {len(purchase_users)}")
    print(f"Carritos abandonados: {len(abandoned_carts)}")
    print(f"Tasa de abandono: {len(abandoned_carts)/len(cart_users)*100:.2f}%")
    
    return abandoned_carts
```

### M√©tricas y KPIs

```python
def calculate_business_kpis():
    """
    Calcula KPIs esenciales del negocio
    """
    kpis = {}
    
    # 1. Conversion Rate
    total_users = df['user_id'].nunique()
    purchasing_users = df[df['event_type'] == 'purchase']['user_id'].nunique()
    kpis['conversion_rate'] = (purchasing_users / total_users) * 100 if total_users > 0 else 0
    
    # 2. Average Order Value
    purchase_data = df[df['event_type'] == 'purchase']
    kpis['avg_order_value'] = purchase_data['price'].mean() if not purchase_data.empty else 0
    
    # 3. Revenue per User
    kpis['total_customers'] = purchasing_users
    kpis['total_revenue'] = purchase_data['price'].sum() if not purchase_data.empty else 0
    kpis['revenue_per_user'] = kpis['total_revenue'] / total_users if total_users > 0 else 0
    
    return kpis

def print_kpi_report():
    """
    Imprime un reporte formateado de KPIs
    """
    kpis = calculate_business_kpis()
    
    print("=" * 50)
    print("           BUSINESS KPIs REPORT")
    print("=" * 50)
    print(f"Conversion Rate:          {kpis['conversion_rate']:.2f}%")
    print(f"Average Order Value:      ${kpis['avg_order_value']:.2f}")
    print(f"Total Customers:          {kpis['total_customers']:,}")
    print(f"Total Revenue:            ${kpis['total_revenue']:,.2f}")
    print(f"Revenue per User:         ${kpis['revenue_per_user']:.2f}")
    print("=" * 50)
```

## üß™ Testing y Validaci√≥n

### Validaci√≥n de Datos

```sql
-- Tests de integridad de datos
-- 1. Verificar que no hay valores nulos cr√≠ticos
SELECT 
    COUNT(*) as total_rows,
    COUNT(user_id) as non_null_users,
    COUNT(event_time) as non_null_times,
    COUNT(event_type) as non_null_events
FROM customers;

-- 2. Verificar rangos de precios v√°lidos
SELECT 
    MIN(price) as min_price,
    MAX(price) as max_price,
    AVG(price) as avg_price,
    COUNT(*) FILTER (WHERE price <= 0) as invalid_prices
FROM customers 
WHERE event_type = 'purchase';

-- 3. Verificar tipos de eventos v√°lidos
SELECT DISTINCT event_type FROM customers;

-- 4. Verificar distribuci√≥n temporal
SELECT 
    MIN(event_time) as earliest_event,
    MAX(event_time) as latest_event,
    COUNT(DISTINCT DATE(event_time)) as unique_days
FROM customers;
```

## üö® Resoluci√≥n de Problemas

### Problemas Comunes de Base de Datos

```bash
# Error de conexi√≥n PostgreSQL
docker-compose logs db

# Reiniciar servicios
docker-compose down
docker-compose up -d

# Verificar estado de contenedores
docker ps

# Limpiar vol√∫menes corruptos
docker-compose down -v
docker volume prune
```

### Problemas de Importaci√≥n de Datos

```sql
-- Verificar encoding de archivos CSV
SHOW server_encoding;

-- Importar con encoding espec√≠fico
COPY customers FROM '/path/to/file.csv' 
WITH (FORMAT csv, HEADER true, ENCODING 'UTF8');
```

## üë®‚Äçüíª Autores

**Desarrollador Principal:**
- **Rub√©n Delicado** - [@rdelicado](https://github.com/rdelicado)
  - üìß rdelicad@student.42malaga.com
  - üè´ 42 M√°laga
  - üìä Especialista en Data Science y Analytics

**√Åreas de Especializaci√≥n:**
- **Database Engineering**: PostgreSQL, SQL avanzado, optimizaci√≥n de queries
- **Data Analysis**: Python, Pandas, an√°lisis estad√≠stico
- **Data Visualization**: Matplotlib, Seaborn, dashboards interactivos
- **ETL Processes**: Automatizaci√≥n de procesos de datos con Shell scripting
- **Business Intelligence**: KPIs, m√©tricas de negocio, an√°lisis de comportamiento

## üìú Licencia

Este proyecto es parte del curr√≠culo de 42 School y est√° disponible para fines educativos. Demuestra competencias avanzadas en ciencia de datos, an√°lisis de datos y ingenier√≠a de datos utilizando tecnolog√≠as industriales est√°ndar.

## üîó Recursos y Referencias

### Data Science y Analytics
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Matplotlib Documentation](https://matplotlib.org/stable/)
- [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)

### SQL y Bases de Datos
- [PostgreSQL Tutorial](https://www.postgresqltutorial.com/)
- [SQL Best Practices](https://www.sqlstyle.guide/)
- [Database Design Principles](https://www.ibm.com/topics/database-design)
- [pgAdmin4 Documentation](https://www.pgadmin.org/docs/)

### An√°lisis de Datos
- [Data Analysis with Python](https://www.coursera.org/learn/data-analysis-with-python)
- [Exploratory Data Analysis](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15)
- [Statistical Analysis Methods](https://www.statology.org/)
- [Business Intelligence Fundamentals](https://www.tableau.com/learn/articles/business-intelligence)

## üöÄ Extensiones Futuras

### Caracter√≠sticas Planeadas
- [ ] **Advanced Machine Learning**: Modelos predictivos con scikit-learn
- [ ] **Real-time Analytics**: Stream processing con Apache Kafka
- [ ] **Advanced Visualization**: Dashboards interactivos con Tableau/PowerBI
- [ ] **Big Data Integration**: Procesamiento con Apache Spark
- [ ] **Cloud Deployment**: Migraci√≥n a AWS/GCP/Azure
- [ ] **API Development**: APIs REST para acceso a datos

### Mejoras T√©cnicas
- [ ] **Data Pipeline Automation**: Airflow para orquestaci√≥n
- [ ] **Data Quality Monitoring**: Frameworks de validaci√≥n autom√°tica
- [ ] **Performance Optimization**: Indexaci√≥n y optimizaci√≥n de queries
- [ ] **Security Enhancement**: Encriptaci√≥n y control de acceso
- [ ] **Backup & Recovery**: Estrategias de respaldo autom√°tico
- [ ] **Monitoring & Alerting**: Sistemas de monitoreo proactivo

---

<div align="center">

*"En la ciencia de datos, la capacidad de extraer insights significativos de datos complejos es lo que transforma informaci√≥n en conocimiento accionable para la toma de decisiones empresariales."*

**Piscine Data Science** demuestra que el dominio integral de tecnolog√≠as de datos, desde bases de datos relacionales hasta an√°lisis estad√≠stico avanzado, forma la base s√≥lida para una carrera exitosa en ciencia de datos y analytics.

</div>